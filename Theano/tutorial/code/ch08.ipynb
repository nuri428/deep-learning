{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STACKED DENOISING AUTOENCODERS (SDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-2f64b1da76ad>, line 366)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-2f64b1da76ad>\"\u001b[1;36m, line \u001b[1;32m366\u001b[0m\n\u001b[1;33m    print '... building the model'\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %load Sda.py\n",
    "\"\"\"\n",
    " This tutorial introduces stacked denoising auto-encoders (SdA) using Theano.\n",
    "\n",
    " Denoising autoencoders are the building blocks for SdA.\n",
    " They are based on auto-encoders as the ones used in Bengio et al. 2007.\n",
    " An autoencoder takes an input x and first maps it to a hidden representation\n",
    " y = f_{\\theta}(x) = s(Wx+b), parameterized by \\theta={W,b}. The resulting\n",
    " latent representation y is then mapped back to a \"reconstructed\" vector\n",
    " z \\in [0,1]^d in input space z = g_{\\theta'}(y) = s(W'y + b').  The weight\n",
    " matrix W' can optionally be constrained such that W' = W^T, in which case\n",
    " the autoencoder is said to have tied weights. The network is trained such\n",
    " that to minimize the reconstruction error (the error between x and z).\n",
    "\n",
    " For the denosing autoencoder, during training, first x is corrupted into\n",
    " \\tilde{x}, where \\tilde{x} is a partially destroyed version of x by means\n",
    " of a stochastic mapping. Afterwards y is computed as before (using\n",
    " \\tilde{x}), y = s(W\\tilde{x} + b) and z as s(W'y + b'). The reconstruction\n",
    " error is now measured between z and the uncorrupted input x, which is\n",
    " computed as the cross-entropy :\n",
    "      - \\sum_{k=1}^d[ x_k \\log z_k + (1-x_k) \\log( 1-z_k)]\n",
    "\n",
    "\n",
    " References :\n",
    "   - P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol: Extracting and\n",
    "   Composing Robust Features with Denoising Autoencoders, ICML'08, 1096-1103,\n",
    "   2008\n",
    "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
    "   Training of Deep Networks, Advances in Neural Information Processing\n",
    "   Systems 19, 2007\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "from logistic_sgd import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from dA import dA\n",
    "\n",
    "\n",
    "# start-snippet-1\n",
    "class SdA(object):\n",
    "    \"\"\"Stacked denoising auto-encoder class (SdA)\n",
    "\n",
    "    A stacked denoising autoencoder model is obtained by stacking several\n",
    "    dAs. The hidden layer of the dA at layer `i` becomes the input of\n",
    "    the dA at layer `i+1`. The first layer dA gets as input the input of\n",
    "    the SdA, and the hidden layer of the last dA represents the output.\n",
    "    Note that after pretraining, the SdA is dealt with as a normal MLP,\n",
    "    the dAs are only used to initialize the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        n_ins=784,\n",
    "        hidden_layers_sizes=[500, 500],\n",
    "        n_outs=10,\n",
    "        corruption_levels=[0.1, 0.1]\n",
    "    ):\n",
    "        \"\"\" This class is made to support a variable number of layers.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: numpy random number generator used to draw initial\n",
    "                    weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                           generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type n_ins: int\n",
    "        :param n_ins: dimension of the input to the sdA\n",
    "\n",
    "        :type n_layers_sizes: list of ints\n",
    "        :param n_layers_sizes: intermediate layers size, must contain\n",
    "                               at least one value\n",
    "\n",
    "        :type n_outs: int\n",
    "        :param n_outs: dimension of the output of the network\n",
    "\n",
    "        :type corruption_levels: list of float\n",
    "        :param corruption_levels: amount of corruption to use for each\n",
    "                                  layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.dA_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                                 # [int] labels\n",
    "        # end-snippet-1\n",
    "\n",
    "        # The SdA is an MLP, for which all weights of intermediate layers\n",
    "        # are shared with a different denoising autoencoders\n",
    "        # We will first construct the SdA as a deep multilayer perceptron,\n",
    "        # and when constructing each sigmoidal layer we also construct a\n",
    "        # denoising autoencoder that shares weights with that layer\n",
    "        # During pretraining we will train these autoencoders (which will\n",
    "        # lead to chainging the weights of the MLP as well)\n",
    "        # During finetunining we will finish training the SdA by doing\n",
    "        # stochastich gradient descent on the MLP\n",
    "\n",
    "        # start-snippet-2\n",
    "        for i in xrange(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the hidden\n",
    "            # layer below or the input of the SdA if you are on the first\n",
    "            # layer\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            # its arguably a philosophical question...\n",
    "            # but we are going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the StackedDAA\n",
    "            # the visible biases in the dA are parameters of those\n",
    "            # dA, but not the SdA\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct a denoising autoencoder that shared weights with this\n",
    "            # layer\n",
    "            dA_layer = dA(numpy_rng=numpy_rng,\n",
    "                          theano_rng=theano_rng,\n",
    "                          input=layer_input,\n",
    "                          n_visible=input_size,\n",
    "                          n_hidden=hidden_layers_sizes[i],\n",
    "                          W=sigmoid_layer.W,\n",
    "                          bhid=sigmoid_layer.b)\n",
    "            self.dA_layers.append(dA_layer)\n",
    "        # end-snippet-2\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs\n",
    "        )\n",
    "\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        # construct a function that implements one step of finetunining\n",
    "\n",
    "        # compute the cost for second phase of training,\n",
    "        # defined as the negative log likelihood\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "    def pretraining_functions(self, train_set_x, batch_size):\n",
    "        ''' Generates a list of functions, each of them implementing one\n",
    "        step in trainnig the dA corresponding to the layer with same index.\n",
    "        The function will require as input the minibatch index, and to train\n",
    "        a dA you just need to iterate, calling the corresponding function on\n",
    "        all minibatch indexes.\n",
    "\n",
    "        :type train_set_x: theano.tensor.TensorType\n",
    "        :param train_set_x: Shared variable that contains all datapoints used\n",
    "                            for training the dA\n",
    "\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a [mini]batch\n",
    "\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during training for any of\n",
    "                              the dA layers\n",
    "        '''\n",
    "\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for dA in self.dA_layers:\n",
    "            # get the cost and the updates list\n",
    "            cost, updates = dA.get_cost_updates(corruption_level,\n",
    "                                                learning_rate)\n",
    "            # compile the theano function\n",
    "            fn = theano.function(\n",
    "                inputs=[\n",
    "                    index,\n",
    "                    theano.Param(corruption_level, default=0.2),\n",
    "                    theano.Param(learning_rate, default=0.1)\n",
    "                ],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin: batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        '''Generates a function `train` that implements one step of\n",
    "        finetuning, a function `validate` that computes the error on\n",
    "        a batch from the validation set, and a function `test` that\n",
    "        computes the error on a batch from the testing set\n",
    "\n",
    "        :type datasets: list of pairs of theano.tensor.TensorType\n",
    "        :param datasets: It is a list that contain all the datasets;\n",
    "                         the has to contain three pairs, `train`,\n",
    "                         `valid`, `test` in this order, where each pair\n",
    "                         is formed of two Theano variables, one for the\n",
    "                         datapoints, the other for the labels\n",
    "\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a minibatch\n",
    "\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during finetune stage\n",
    "        '''\n",
    "\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = [\n",
    "            (param, param - gparam * learning_rate)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='train'\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='test'\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='valid'\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in xrange(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score\n",
    "\n",
    "\n",
    "def test_SdA(finetune_lr=0.1, pretraining_epochs=15,\n",
    "             pretrain_lr=0.001, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=1):\n",
    "    \"\"\"\n",
    "    Demonstrates how to train and test a stochastic denoising autoencoder.\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used in the finetune stage\n",
    "    (factor for the stochastic gradient)\n",
    "\n",
    "    :type pretraining_epochs: int\n",
    "    :param pretraining_epochs: number of epoch to do pretraining\n",
    "\n",
    "    :type pretrain_lr: float\n",
    "    :param pretrain_lr: learning rate to be used during pre-training\n",
    "\n",
    "    :type n_iter: int\n",
    "    :param n_iter: maximal number of iterations ot run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path the the pickled dataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches /= batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    # start-snippet-3\n",
    "    numpy_rng = numpy.random.RandomState(89677)\n",
    "    print '... building the model'\n",
    "    # construct the stacked denoising autoencoder class\n",
    "    sda = SdA(\n",
    "        numpy_rng=numpy_rng,\n",
    "        n_ins=28 * 28,\n",
    "        hidden_layers_sizes=[1000, 1000, 1000],\n",
    "        n_outs=10\n",
    "    )\n",
    "    # end-snippet-3 start-snippet-4\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print '... getting the pretraining functions'\n",
    "    pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "    print '... pre-training the model'\n",
    "    start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "    corruption_levels = [.1, .2, .3]\n",
    "    for i in xrange(sda.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in xrange(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in xrange(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                         corruption=corruption_levels[i],\n",
    "                         lr=pretrain_lr))\n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print >> sys.stderr, ('The pretraining code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "    # end-snippet-4\n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print '... getting the finetuning functions'\n",
    "    train_fn, validate_model, test_model = sda.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    print '... finetunning the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 10 * n_train_batches  # look as this many examples regardless\n",
    "    patience_increase = 2.  # wait this much longer when a new best is\n",
    "                            # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'on iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "    print >> sys.stderr, ('The training code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_SdA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (SdA.py, line 365)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\".\\SdA.py\"\u001b[1;36m, line \u001b[1;32m365\u001b[0m\n\u001b[1;33m    print '... building the model'\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import SdA\n",
    "SdA.test_SdA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
